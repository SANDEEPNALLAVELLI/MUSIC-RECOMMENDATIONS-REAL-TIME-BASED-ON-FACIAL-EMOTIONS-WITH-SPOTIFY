# Mood Based Music Recommendation System using Facial Emoton Model 

This project is the development of a prototype model of a cutting-edge Music Recommendation System.
The project unfolds through distinct phases, each contributing to the overall functionality and user experience:

**1. Data Collection and Extraction:** 
  Utilizes the **FER-2013 datset** for face images and leveraging the **Spotify Web API**, the system aggregates a diverse array of musical tracks.
   
**2. Data Cleaning and Preprocessing:**

  | FER Data Preprocessing | Description | 
  | ------------- | ------------- |
  | Data Reduction | Reducing to interested emotions|
  | Image Conversion | Converting image to raw pixel values |
  | Data Upsampling | Upsampling the minority class |
  | Data Reshaping | Reshaping data to one dimension |
  | Data Preparation | Extracting input and target variables |
  | Data Splits | Splitting the data for training and testing |
  | Data Normalization | Scaling down the values of data |

  | Song Data Preprocessing | Description |
  | ------------- | ------------- |
  | Drop Duplicates |Dropping the duplicate data |
  | Data Normalization | Scaling down the numeric features |
  | Genre Clustering | Identifying the genre of a track |
  | Data Storing | Storing the data into database |

**3. Model Building:**
  At the core a sophisticated recommendation model that incorporates state-of-the-art techniques. The process involves the utilization of a custom Convolutional Neural Network (CNN) model, referred to as **FERNET**, for precise emotion classification.

**4. Model Optimization:**
  Rigorous testing and validation of the emotion classification model guarantee its accuracy and reliability ensuring robust and dependable performance with the **Accuracy = 96%**.

**5. Application Development:**
  The culmination of the project results in the creation of the **Mood-Music App**, which seamlessly integrates the FER module, Emotion Recognition module, and Songs Recommendation module. This user-friendly application harnesses the power of real-time emotion analysis to provide a personalized and immersive music discovery experience.


# Mood-music-app...

This is an Application which consists of three different modules. 

**FER Module** is a video streaming module that gets the user data from a webcam.

**Emotion Recognition** predicts the binary class of the universal emotios such as happy or sad by inputting a user face image and the Fernet classifies the output.

**Songs Recommendation** yeilds the songs that are relevant to the genre classified such as sombre or cheerful according to the emotions classified.

This app tries to personalize the song recommendations by taking the user facial emotions into account.

## Features
 **Real time expression detection and song recommendations
 **Playlists fetched from Spotify using API
 **Neumorphism UI for website.

 # Running the app:
 Flask:
 Run pip install -r requirements.txt to install all dependencies.
In Spotipy.py enter your credentials generated by your Spotify Developer account in 'auth_manager'. Note: - This is only required if you want to update recommendation playlists. Also uncomment import statement in 'camera.py'.
Run python app.py and give camera permission if asked.

## Tech Stack

Keras
Tensorflow
Spotipy
Tkinter (For testing)
Flask

## Dataset :
The dataset used for this project is the famous FER2013 dataset. Models trained on this dataset can classify 7 emotions. The dataset can be found here.

Note that the dataset is highly imbalanced with happy class having maxiumum representation. This might be a factor resulting in okaysish accuracy after training.

## Model Architecture
The model architecture is a sequential model consisting of Conv2d, Maxpool2d, Dropout and Dense layers:
Conv2D layers throughout the model have different filter size from 32 to 128, all with activation 'relu'
Pooling layers have pool size (2,2)
Dropout is set to 0.25 as anything above results in poor performance
Final Dense layer has 'softmax' activation for classifying 7 emotions
Used 'categorical_crossentropy' for loss with 'Adam' optimizer with 'accuracy' metric
Note:- Tried Implementing various other models like VGG16 but accuracy was far too low. This model architecture gives good enough accuracy. A bit more tinkering with hyper parameters might lead to a better accuracy

## Project Components :

**Spotipy is a module for establishing connection to and getting tracks from Spotify using Spotipy wrapper.
**haarcascade is for face detection.
**camera.py is the module for video streaming, frame capturing, prediction and recommendation which are passed to main.py.
**main.py is the main flask application file.
**index.html in 'templates' directory is the web page for the application. Basic HTML and CSS.
**utils.py is an utility module for video streaming of web camera with threads to enable real time detection.
**train.py is the script for image processing and training the model.

## Further Work:

** Instead of CSVs, create a databse and connect it to application. The DB will fetch songs for recommendations and new songs can be updated directly onto database
** Add a feature which will update specified playlists for better and more recent recommendations, a specific day over a fixed duration say every sunday and append it to database
** Directly play the song or redirect to the song on Spotify when user clicks on it.
** Rewrite code such that Video Streaming is done on client side instead of server side so as it make the app deployable
Note: Model accuracy is not that great. It is ~66%. Further training and finetuning required. May try Vision Transformer Model.

## Screenshots:
![perfect](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/18fb7cc1-5608-449f-80db-712a9551171f)


![Screenshot 2023-11-16 212410](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/420d7de5-3ab4-4c5e-b7f6-2cdd872863ce)




![Screenshot 2023-11-16 214345](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/a2a2e30c-48d3-4a53-a119-77f98e26a742)


![Screenshot 2023-11-16 214433](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/75a6a3da-541c-4023-9e32-66d7f36cc8bf)



![Screenshot 2023-11-16 214807](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/9d133430-1f16-4f2f-bea8-ea364243c0a0)


![Screenshot 2023-11-16 214747](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/21e80202-3d57-4554-ad6d-be94afa9ad35)

![Screenshot 2023-11-16 215325](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/2b386dc3-664d-4eb4-b4e8-975b0b2e9797)



![Screenshot 2023-11-16 215637](https://github.com/SANDEEPNALLAVELLI/MUSIC-RECOMMENDATIONS-REAL-TIME-BASED-ON-FACIAL-EMOTIONS-WITH-SPOTIFY/assets/131253322/9117ce82-589e-420f-aef3-47b4422a0c01)





